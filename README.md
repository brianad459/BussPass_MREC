# ğŸšŒ BussPass MREC: Gesture-Controlled Interactive Signage

### A Real-Time Rockâ€“Paperâ€“Scissors and Smart Transit Display  
Powered by **YOLOv11**, **MediaPipe**, and **TensorFlow Lite**

---

## ğŸš€ Overview

**BussPass MREC** is an interactive digital signage system designed to make public spaces smarter and more engaging.  
It detects nearby pedestrians using **YOLOv11** and recognizes **hand gestures** with **MediaPipe** + a **custom TensorFlow Lite model**.

When a person gestures "game" in ASL, the system invites them to play a real-time **Rockâ€“Paperâ€“Scissors** game using their webcam feed â€” complete with visual overlays, icons, countdowns, and voice feedback.  

Outside of gameplay, it can also display **traffic and route information** from APIs like HERE, GraphHopper, and TomTom â€” turning a simple bus stop into an interactive smart hub.

---

## âœ¨ Features

- ğŸ§  **Gesture Recognition** â€” Detects â€œgameâ€, â€œrockâ€, â€œpaperâ€, and â€œscissorâ€ gestures in real time  
- ğŸ•¹ï¸ **Interactive Gameplay** â€” Real-time countdown, gesture comparison, and on-screen score tracking  
- ğŸ§ **Person Detection (YOLOv11)** â€” Identifies people nearby and labels â€œPLAYERâ€ above the current participant  
- ğŸ—£ï¸ **Voice Feedback (Speaker Module)** â€” Announces gameplay prompts and results using text-to-speech  
- ğŸªŸ **Animated Visual Overlays** â€” Sprites, pulse animations, icons, and round indicators drawn with OpenCV  
- ğŸ›£ï¸ **Traffic Integration (API)** â€” Displays live traffic routes and accident information  
- ğŸ” **Automatic Restart** â€” Gesture â€œgameâ€ again to replay after three rounds  

---

## ğŸ§© Tech Stack

| Component | Technology |
|------------|-------------|
| Object Detection | YOLOv11 (Ultralytics) |
| Gesture Tracking | MediaPipe Hands |
| Gesture Classification | TensorFlow Lite (Custom Model) |
| Programming Language | Python 3.10+ |
| Visualization | OpenCV |
| Voice Interaction | pyttsx3 / Speaker.py |
| Data Handling | Pandas, NumPy |
| APIs | GraphHopper / HERE / TomTom |

---

## ğŸ§  Model Training

Gesture recognition was trained using custom CSV datasets captured via webcam:
`python
label, x0, y0, z0, ... x20, y20, z20
rock, ...
paper, ...
scissor, ...
game, ...

The training pipeline (TensorFlow + Scikit-Learn):

Scales input features using StandardScaler

Encodes labels with LabelEncoder

Trains a dense neural network (256-128-softmax)

Converts to .tflite for real-time inference

Resulting files:

gesture_model_new.tflite
gesture_label_encoder.pkl
gesture_input_scaler.pkl

Installation:
git clone https://github.com/brianad459/BussPass_MREC.git
cd BussPass_MREC

Create & activate a virtual environment:'
python -m venv .venv
.\.venv\Scripts\activate

Install dependencies:
pip install -r requirements.txt

Run The Project:
python YOLO11_testing2.py


